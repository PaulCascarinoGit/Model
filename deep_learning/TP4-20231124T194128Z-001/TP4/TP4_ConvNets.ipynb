{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"colab":{"name":"TP4_ConvNets.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BNA5NPr1Wh2J"},"source":["# TP 4 - Deep learning for computer vision\n","\n","---\n","This notebook contains the code samples found in Chapter 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n","\n","---\n","\n","Convolutional neural networks are very similar to regular neural networks: they are made up of layers that have trainable parameters. So what does change? ConvNets make the explicit assumption that the inputs are images, which allows them to encode certain properties into the architecture. This makes the forward propagation more efficient to implement and vastly reduce the amount of parameters in the network.\n","\n","#### Fully-connected layers\n","\n","Neural networks transform its input data through a series of layers. Each layer is made up of neurons, and each of them is fully connected to all neurons in the previous layer. Due to the high number of neurons and connections, the fully-connected structure clearly does not scale well to images. For example, an image of respectable size, say 200x200x3, would lead to neurons that have 200x200x3 = 120'000 weights. We would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n","\n","![fcn.jpeg](https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0402.png)\n","\n","#### Convolutional layers\n","\n","ConvNets take advantage of the fact that the input consists of images, and they constrain the architecture in a more sensible way. To do so, they make use of convolutional layers. The fundamental difference is this: fully-connected layers learn global patterns in the inputs, whereas convolution layers learn local patterns in small 2D windows of the inputs. More specifically, a convolution layer operates over 3D tensors with two spatial axes and a depth axis (height, width, channels). The convolution operation extracts patches from its input, and applies the same transformation to all of these patches. The output is still a 3D tensor, but its dimensions depend on the layer's hyper-parameters, specified by the *kernel size* and the *number of kernels*.\n","\n","![conv2.jpeg](https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-26-at-1.44.58-PM.png)\n","\n","#### ConvNet architecture\n","\n","ConvNets mainly use three types of layers: convolutional (CONV), pooling (POOL), fully-connected (FC). The figure below shows a concrete example of ConvNet architecture. The first layer (left) stores the raw image pixels, whereas and the last layer (right) stores the class probabilities. The activation of each hidden layer along the processing path is shown as a column.\n","\n","![convnet.jpeg](https://editor.analyticsvidhya.com/uploads/90650dnn2.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"rmqMMKydWh2L"},"source":["# 1. Introduction to ConvNets\n","\n","Let's take a practical look at a very simple convnet for MNIST digit classification, a task that you have already been through using a fully-connected network."]},{"cell_type":"markdown","metadata":{"id":"14hRt6xvWh2N"},"source":["#### Input layer\n","\n","A ConvNet takes as input a tensor of shape `(image_height, image_width, image_channels)`. In our case, we configure our ConvNet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via passing the argument `input_shape=(28, 28, 1)` to our first layer."]},{"cell_type":"code","metadata":{"id":"8PEp46gRWh2Q"},"source":["input_dim = (28, 28, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMMC4Y62Wh2S"},"source":["#### Convolutional layers\n","\n","A ConvNet always starts off with convolutional and pooling layers. In our case, we stack three convolutional layers, alternated with pooling layers."]},{"cell_type":"code","metadata":{"id":"IJpHVy3dWh2S"},"source":["from keras import layers\n","from keras import models\n","\n","model = models.Sequential()\n","model.add( layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_dim) )\n","model.add( layers.MaxPooling2D((2, 2)) )\n","model.add( layers.Conv2D(64, (3, 3), activation='relu') )\n","model.add( layers.MaxPooling2D((2, 2)) )\n","model.add( layers.Conv2D(64, (3, 3), activation='relu') )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLeWDC_wWh2Y"},"source":["Let's display the architecture of our ConvNet so far."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BL86psyNWh2Z","executionInfo":{"status":"ok","timestamp":1612363862473,"user_tz":-60,"elapsed":1269,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}},"outputId":"f824d0f1-0059-4bf5-f83b-4a4374b03a78"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","=================================================================\n","Total params: 55,744\n","Trainable params: 55,744\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cBDeYwBaWh2a"},"source":["You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to the `Conv2D` layers."]},{"cell_type":"markdown","metadata":{"id":"FKCZ_q0LWh2b"},"source":["#### Fully-connected layers\n","\n","The next step is to feed our last layer's output, a tensor of shape `(3, 3, 64)`, into a fully-connected classifier. However, such a classifier processes 1D vectors, whereas our current output is a 3D tensor. So first, we have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top. We are going to do 10-way classification, so we use a final layer with 10 outputs and a `softmax` activation. "]},{"cell_type":"code","metadata":{"id":"lX7RvsqHWh2c"},"source":["model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(10, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NX_caVVaWh2e"},"source":["Now here's what our network looks like:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQO_gmAoWh2f","executionInfo":{"status":"ok","timestamp":1612363868879,"user_tz":-60,"elapsed":929,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}},"outputId":"9a3fe017-6486-4fb8-c0f9-ebeb1ae407c0"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 576)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                36928     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                650       \n","=================================================================\n","Total params: 93,322\n","Trainable params: 93,322\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3oDKcnO9Wh2g"},"source":["As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers."]},{"cell_type":"markdown","metadata":{"id":"XQz4eI5HWh2g"},"source":["#### Training\n","\n","Now, let's train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the previous MNIST example.\n","\n","First, we load and preprocess the data. Remember that you must **ALWAYS normalize** your data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7v7EjhXWh2h","executionInfo":{"status":"ok","timestamp":1612363872684,"user_tz":-60,"elapsed":1253,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}},"outputId":"4c709ce3-e3ca-4f4c-8c10-b9421e987d55"},"source":["from keras.datasets import mnist\n","from keras.utils import to_categorical\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","train_images = train_images.reshape((60000, 28, 28, 1))\n","test_images  =  test_images.reshape((10000, 28, 28, 1))\n","\n","train_images = train_images.astype('float32') / 255\n","test_images  =  test_images.astype('float32') / 255\n","\n","train_labels = to_categorical(train_labels)\n","test_labels  = to_categorical( test_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CQfk3-JEWh2h"},"source":["Second, we train the network using the cross-entropy loss function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-p06a3ycWh2l","executionInfo":{"status":"ok","timestamp":1612363901112,"user_tz":-60,"elapsed":23016,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}},"outputId":"dd56c550-ec7e-4ac1-95be-17e0d469f1cc"},"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","model.fit(train_images, train_labels, epochs=5, batch_size=64)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","938/938 [==============================] - 10s 3ms/step - loss: 0.4337 - accuracy: 0.8652\n","Epoch 2/5\n","938/938 [==============================] - 3s 3ms/step - loss: 0.0503 - accuracy: 0.9844\n","Epoch 3/5\n","938/938 [==============================] - 3s 3ms/step - loss: 0.0349 - accuracy: 0.9890\n","Epoch 4/5\n","938/938 [==============================] - 3s 3ms/step - loss: 0.0256 - accuracy: 0.9920\n","Epoch 5/5\n","938/938 [==============================] - 3s 3ms/step - loss: 0.0214 - accuracy: 0.9928\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f2fb3842208>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"u-FsOVKnWh2n"},"source":["Let's evaluate the model on the test data:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XG1TctG2Wh2o","executionInfo":{"status":"ok","timestamp":1612363909405,"user_tz":-60,"elapsed":1651,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}},"outputId":"7a59ce6d-a03d-4054-e146-4003727349b3"},"source":["test_loss, test_acc = model.evaluate(test_images, test_labels)\n","\n","print(\"Test accuracy:\", test_acc*100, \"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9911\n","Test accuracy: 99.1100013256073 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cMxNQA6tWh2q"},"source":["While our previous fully-connected network had a test accuracy of 97%, our basic convnet has a test accuracy of 99%."]},{"cell_type":"markdown","metadata":{"id":"PZqCvrTEWh2q"},"source":["## ===== Exercise =====\n","\n","Grab the relevant functions from TP 2, and test your convolutional network for handwritten digit recognition. *Do you see any difference in the performance?*"]},{"cell_type":"code","metadata":{"id":"tPgQbiwFWh2r"},"source":["# ADD CODE HERE"],"execution_count":null,"outputs":[]}]}