{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - A first look at neural networks\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains the code samples found in Chapter 3 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n",
    "\n",
    "----\n",
    "\n",
    "A machine-learning system is trained rather than being explicitly programmed. It’s presented with many examples relevant to a task, and it finds the statistical structure in these examples that eventually allows the system to come up with rules for automating the task. Specifically, machine learning needs three things: \n",
    " 1. input data points, \n",
    " - examples of the expected output, \n",
    " - a way to measure whether the algorithm is doing a good job. \n",
    " \n",
    "The above items are needed to determine the distance between the algorithm’s current output and its expected output. This measurement is used as a feedback signal to adjust the way the algorithm works. This adjustment step is what we call *learning*.\n",
    "\n",
    "#### Learning representations from data\n",
    "\n",
    "A machine-learning model transforms its input data into meaningful outputs, a process that is “learned” from exposure to known examples of inputs and outputs. Therefore, the central problem in machine learning is to meaningfully\n",
    "transform data: in other words, to learn useful representations of the input data, in order to get us closer to the expected output. Before we go any further: what’s a representation? At its core, it’s a different way to look at data. \n",
    "\n",
    "All machine-learning algorithms consist of automatically finding transformations that turn data into more useful representations for a given task. These operations can be linear projections, translations, nonlinear operations, and so on. Machine-learning algorithms aren’t usually creative in finding these transformations; they’re merely searching through a predefined set of operations, called a *hypothesis space*. That’s what machine learning is: searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal.\n",
    "\n",
    "#### Deep learning\n",
    "\n",
    "Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive *layers* of increasingly meaningful representations. These layered representations are (almost always) learned via models called *neural networks*. Modern deep learning often involves tens or even hundreds of successive layers of representations, and they’re all learned automatically from exposure to training data. \n",
    "\n",
    "What do the representations learned by a deep-learning algorithm look like? Let’s examine how a network transforms an image in order to recognize its content. As you can see in the figure below, the network transforms the input image into representations that are increasingly different from the original image, and increasingly informative\n",
    "about the final result. You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters, and comes out increasingly purified with regard to some task.\n",
    "\n",
    "![network.png](attachment:network.png)\n",
    "\n",
    "\n",
    "At this point, you know that machine learning is about mapping inputs to targets, which is done by observing many examples of input and targets. You also know that deep neural networks do this input-to-target mapping via a deep sequence of simple data transformations (layers), and that these data transformations are learned by exposure to examples. Now let’s look at how this learning happens, concretely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Anatomy of a neural network\n",
    "\n",
    "We will now take a look at a first concrete example of a neural network, which makes use of the Python library [Keras](https://keras.io). \n",
    "\n",
    "The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). The dataset we will use is the MNIST dataset, which contains a set of 60,000 training images, plus 10,000 test images. You can think of \"solving\" MNIST  as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. As you become a machine \n",
    "learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on.\n",
    "\n",
    "Our workflow will be as follow.\n",
    "\n",
    " 1. Load the dataset. \n",
    " - Build the neural network.\n",
    " - Present the network with the training data, `train_images` and `train_labels`, in order to learn the association between images and labels. \n",
    " - Ask the network to produce predictions for `test_images`, and verify if these predictions match the labels from `test_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e8637cd44d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboston_housing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist, boston_housing\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2413b0c6361a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_images` and `train_labels` form the \"training set\", the data that the model will learn from. The model will then be tested on the \n",
    "\"test set\", `test_images` and `test_labels`. Our images are encoded as Numpy arrays (or tensors), and the labels are simply an array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.\n",
    "\n",
    "Let's have a look at the training data. In general, the first axis (axis=0) in all data tensors you’ll come across in deep learning will be the samples axis (sometimes called the samples dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, deep-learning models don’t process an entire dataset at once; rather, they break the data into small batches. Concretely, here’s one batch of our MNIST digits, with batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "inputs = train_images[0:batch_size,:,:]\n",
    "labels = train_labels[0:batch_size]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(np.hstack([inputs[i,:,:] for i in range(inputs.shape[0])]))\n",
    "plt.title(\"Inputs\")\n",
    "plt.show()\n",
    "print(\"Labels: \" + str(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here’s another batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 15\n",
    "\n",
    "inputs = train_images[n*batch_size : (n+1)*batch_size,:,:]\n",
    "labels = train_labels[n*batch_size : (n+1)*batch_size]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(np.hstack([inputs[i,:,:] for i in range(inputs.shape[0])]))\n",
    "plt.title(\"Inputs\")\n",
    "plt.show()\n",
    "print(\"Labels: \" + str(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network layers\n",
    "\n",
    "The building block of neural networks is the **layer**, which can be thought of as a filter for data. Precisely, a layer is a data-processing module that takes as input one or more tensors, and outputs one or more transformed tensors. Some layers are stateless, but more frequently layers have a state: the **weights** (or parameters), which in essence are a bunch of numbers. The transformation implemented by a layer is parameterized by its weights. \n",
    "\n",
    "For example, a fully-connected layer looks like this in Keras:\n",
    "\n",
    "```layers.Dense(512, activation='relu')```\n",
    "\n",
    "This layer can be interpreted as a function, which takes as input a 2D tensor, and returns a transformed 2D tensor. Assuming that `W` and `b` are the layer's weighs, the function is as follows: \n",
    "\n",
    "```output = relu(input.dot(W) + b)```\n",
    "\n",
    "Let’s unpack this. We have three tensor operations here: a dot product (dot) between the input tensor and the 2D tensor `W`; an addition (+) between the resulting 2D tensor and the vector `b`; and, finally, a ReLU operation which computes `max(x, 0)` element-wise.\n",
    "\n",
    "The argument being passed to a fully-connected layer is the number of \"hidden units\". Having 512 hidden units means that the weight matrix `W` will have shape `(input_dimension, 512)`. That is, the dot product with `W` will project the input data onto a 512-dimensional representation space (and then we would add the bias vector `b` and apply the `relu` operation). Intuitively, the number of hidden units controls \"how much freedom you are allowing the network to have when learning internal representations\". Having more hidden units allows your network to learn more complex representations, but it makes your network more computationally expensive, and may lead to learning unwanted patterns.\n",
    "\n",
    "Neural networks consist of layers clipped together to implement a form of progressive *data distillation*. The network topology constrains the *hypothesis space* to a specific series of operations, mapping input data to output data. In this context, learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets. \n",
    "\n",
    "The most common network is a linear stack of layers, mapping a single input to a single output. But even so, there are  two key architecture decisions to be made: how many layers to use, and how many hidden units to chose for each layer. Picking the right network architecture is more an art than a science. Although there are some best practices and principles you can rely on, only practice can help you become a proper neural-network architect. \n",
    "\n",
    "![layer.png](attachment:layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### MNIST digit classification \n",
    "\n",
    "To solve MNIST, we use a network with two `Dense` layers. The first one is a 512-dimensional ReLU layer, whereas the second (and last) one is a 10-way \"softmax\" layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes. The code below is the way of building such a network in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax')) # REMARK: softmax is for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and optimizer\n",
    "\n",
    "Initially, right after the network architecture is defined, the parameters of all layers are filled with small random values (a step called random initialization). So, the network merely implements a series of random transformations. What comes next is to gradually adjust these parameters, based on the available data. This gradual adjustment is basically the *training* that machine learning is all about. \n",
    "\n",
    "To make our network ready for training, we need to pick two more things.\n",
    "\n",
    "- **Loss function**. To control the output of a neural network, we need to be able to measure how far this output is from what you expected. This is the job of the loss function of the network. The loss function takes the prediction of the network and the true target (what you wanted the network to output) and computes a distance score, capturing how well the network has done on this specific example.\n",
    "\n",
    "\n",
    "- **Optimizer.** The fundamental trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example. This adjustment is the job of the *optimizer*, which implements what’s called the *Backpropagation* algorithm.\n",
    "\n",
    "\n",
    "Choosing the right loss function for the right problem is extremely important, as a network will take any shortcut it can to minimize the loss. So, if the loss doesn’t fully correlate with success for the task at hand, our network will end up doing things you may not have wanted. Fortunately, when it comes to common problems such as classification and regression, there are simple guidelines we can follow to choose the correct loss:\n",
    " - **Binary cross-entropy** for a two-class classification,\n",
    " - **Categorical cross-entropy** for a many-class classification problem, \n",
    " - **Mean squared error** for a regression problem.\n",
    " \n",
    "![loss.png](attachment:loss.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### MNIST digit classification\n",
    "\n",
    "As part of the \"compilation\" step, we prepare the training as follows:\n",
    "\n",
    " - The loss function is the categorical cross-entropy, as we are dealing with multiclass classification; \n",
    " - The optimizer is ADAM, an accelerated version of stochastic gradient descent.\n",
    "\n",
    "Additionally, we ask the network to monitor a metric during training: the classification accuracy. (Note that `metrics` is an optional argument.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is where the network parameters are gradually adjusted. Within this loop, the following steps are repeated as long as necessary:\n",
    " 1. Draw a batch of training samples `x` and corresponding targets `y`.\n",
    " - Run the network on `x` to obtain predictions `y_pred`.\n",
    " - Compute the loss of the network on the batch, a measure of the mismatch between `y_pred` and `y`.\n",
    " - Update all weights of the network in a way that slightly reduces the loss on this batch.\n",
    " \n",
    "You’ll eventually end up with a network that has a very low loss on its training data: a low mismatch between predictions `y_pred` and expected targets `y`. The network has “learned” to map its inputs to outputs that are as close as they can be to the correct targets.\n",
    "\n",
    "Step 1 sounds easy enough. Steps 2 and 3 are merely the application of a handful of tensor operations, so you could implement these steps purely from what you learned so far. The difficult part is step 4: updating the network’s weights. Given an individual weight coefficient in the network, how can you compute whether the coefficient should be increased or decreased, and by how much?\n",
    "\n",
    "One naive solution would be to freeze all weights in the network except the one scalar coefficient being considered, and try different values for this coefficient. But such an approach would be horribly inefficient, because you’d need to compute multiple forward passes (which are expensive) for every individual coefficient (of\n",
    "which there are many, usually thousands and sometimes up to millions). A much better approach is to take advantage of the fact that all operations used in the network are differentiable, and compute the gradient of the loss with regard to the network’s coefficients. You can then move the coefficients in the opposite direction from the gradient, thus decreasing the loss.\n",
    "\n",
    "![training.png](attachment:training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### MNIST digit classification\n",
    "\n",
    "Before training, we preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in  the `[0, 1]` interval. Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with  values in the `[0, 255]` interval. We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to categorically encode the labels. This step is always required in multi-class classification, as the categoral cross-entropy loss expects the labels to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network: \n",
    "we \"fit\" the model to its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = network.fit(train_images, train_labels, epochs=10, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two quantities are being displayed during training: the \"loss\" of the network over the training data, and the accuracy of the network over the training data (as we specified a metric as part of the compilation step).\n",
    "\n",
    "We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data.\n",
    "\n",
    "Note that the call to `model.fit()` returns a History object. This object has a member history, which is a dictionary containing data about everything that happened during training. Let's take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains 2 entries, one per metric that was being monitored during training: \n",
    " - `loss` - loss function\n",
    " - `acc` - classification accuracy.\n",
    " \n",
    "Let's use Matplotlib to plot the training loss, as well as the training accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, (ax1,ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.set_title('Training loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "\n",
    "ax2.plot(history.history['acc'],'r')\n",
    "ax2.set_title('Training accuracy')\n",
    "ax2.set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That's what you would expect when running gradient descent optimization -- the quantity you are trying to minimize should get lower with every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "After having trained a network, you will want to use it in a practical setting. You can generate the probability that the input digit image belongs to one of our 10 digit classes by using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = network.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the `predict` method returns a probability distribution over all 10 classes. Each entry in `predictions` is indeed a vector of length 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients in this vector sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest entry is the predicted class, as it is the class with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, here are the predictions for a batch of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 27\n",
    "inputs  = test_images[n*batch_size : (n+1)*batch_size,:].reshape(batch_size, 28, 28)\n",
    "outputs = predictions[n*batch_size : (n+1)*batch_size,:]\n",
    "\n",
    "f, ax = plt.subplots(batch_size,2, figsize=(10,4*batch_size))\n",
    "for i in range(batch_size):\n",
    "    ax[i,0].imshow(inputs[i,:,:])\n",
    "    ax[i,0].set_title(\"Digit\")\n",
    "    ax[i,1].bar(range(10), outputs[i,:])\n",
    "    ax[i,1].set_title(\"Prediction (probabilities)\")\n",
    "    ax[i,1].set_xticks(range(10))\n",
    "    ax[i,1].set_ylim((0,1.1))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Now let's check that our model performs well on the test set too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test set accuracy turns out to be 97.8% -- that's quite a bit lower than the training set accuracy (98.9%). \n",
    "This gap between training accuracy and test accuracy is an example of \"overfitting\", \n",
    "the fact that machine learning models tend to perform worse on new data than on their training data. \n",
    "Overfitting will be a central topic in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== Exercise =====\n",
    "\n",
    "* We were using 1 hidden layer. Try to use more hidden layers, or no hidden layer at all, and see how it affects the test accuracy.\n",
    "* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
    "* Try to use the `mse` loss function instead of `categorical_crossentropy`.\n",
    "* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.\n",
    "\n",
    "These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be improved!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Handwritten Digit Recognition\n",
    "\n",
    "In this section, you will build an algorithm for handwritten digit recognition. The workflow consists of the following steps. \n",
    "\n",
    " 1. Take an image as the input.\n",
    "\n",
    " - Find all the handwritten digits.\n",
    "\n",
    " - Convert each digit into its numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit extraction\n",
    "\n",
    "The fist step is to highlight the digits contained in the input image. This can be done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the image\n",
    "im = cv2.imread(\"data/photo1.jpg\")\n",
    "\n",
    "# Convert the image to grayscale and apply Gaussian filtering\n",
    "im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "im_gray = cv2.GaussianBlur(im_gray, (5, 5), 0)\n",
    "\n",
    "# Threshold the image\n",
    "ret, im_th = cv2.threshold(im_gray, 90, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Find contours in the image\n",
    "im2, ctrs, hier = cv2.findContours(im_th.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Get rectangles containing each contour\n",
    "rects = [cv2.boundingRect(ctr) for ctr in ctrs]\n",
    "\n",
    "# draw a rectangle around the detected digits\n",
    "for rect in rects:\n",
    "    cv2.rectangle(im, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (0, 255, 0), 3)\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code below to extract and display the single digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, rect in enumerate(rects):\n",
    "    \n",
    "    # Exctract the digit\n",
    "    leng = int(rect[3] * 1.6)\n",
    "    pt1 = int(rect[1] + rect[3] // 2 - leng // 2)\n",
    "    pt2 = int(rect[0] + rect[2] // 2 - leng // 2)\n",
    "    roi = im_th[pt1:pt1+leng, pt2:pt2+leng]\n",
    "    \n",
    "    # Fit it to a 28x28 image\n",
    "    roi = cv2.resize(roi, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "    roi = cv2.dilate(roi, (3, 3))\n",
    "    \n",
    "    # show\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(roi, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit classification\n",
    "\n",
    "The second step is to convert each handwritted digit into its corresponding numerical value. To do so, you will use the neural network trained on MNIST dataset. The following function puts everything together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== Exercise =====\n",
    "\n",
    "In the following functoin, complete the lines with the comment `# ADD CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_digits(image_name, model):\n",
    "    \n",
    "    # read the image\n",
    "    im = cv2.imread(image_name)\n",
    "    \n",
    "    # Convert the image to grayscale and apply Gaussian filtering\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    im_gray = cv2.GaussianBlur(im_gray, (5, 5), 0)\n",
    "\n",
    "    # Threshold the image\n",
    "    ret, im_th = cv2.threshold(im_gray, 90, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Find contours in the image\n",
    "    im2, ctrs, hier = cv2.findContours(im_th.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Get rectangles containing each contour\n",
    "    rects = [cv2.boundingRect(ctr) for ctr in ctrs]\n",
    "\n",
    "    # extract the digits\n",
    "    for rect in rects:\n",
    "        \n",
    "        # Draw the rectangles\n",
    "        cv2.rectangle(im, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (0, 255, 0), 3)\n",
    "    \n",
    "        # Make the rectangular region around the digit\n",
    "        leng = int(rect[3] * 1.6)\n",
    "        pt1 = int(rect[1] + rect[3] // 2 - leng // 2)\n",
    "        pt2 = int(rect[0] + rect[2] // 2 - leng // 2)\n",
    "        roi = im_th[pt1:pt1+leng, pt2:pt2+leng]\n",
    "    \n",
    "        if roi.shape[0] < 28 or roi.shape[1] < 28:\n",
    "            continue\n",
    "        \n",
    "        # Resize the image\n",
    "        roi = cv2.resize(roi, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "        roi = cv2.dilate(roi, (3, 3))\n",
    "        \n",
    "        # classify the image\n",
    "        roi = roi.astype('float32')\n",
    "        \n",
    "        # 1. Reshape and normalize 'roi'\n",
    "        roi = ... # ADD CODE HERE\n",
    "        \n",
    "        # 2. Use 'model' to make a prediction\n",
    "        probas = ... # ADD CODE HERE\n",
    "        \n",
    "        # 3. Based on the prediction, determine the class (a number between 0 and 9)\n",
    "        number = ... # ADD CODE HERE\n",
    "        \n",
    "        # draw the digit\n",
    "        cv2.putText(im, str(number), (rect[0], rect[1]-5), cv2.FONT_HERSHEY_DUPLEX, 1.5, (255, 0, 0), 3)\n",
    "\n",
    "    # show the result\n",
    "    plt.imshow(im)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detect_digits(\"data/photo1.jpg\", network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detect_digits(\"data/photo2.jpg\", network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== Exercise =====\n",
    "\n",
    "- Write down some numbers on a paper, and take a picture with our phone.\n",
    "\n",
    "- Use your network to recognize the digits on the scanned photos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "\n",
    "Here's what you should take away from this tutorial.\n",
    "\n",
    "* There's usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it into a neural network.\n",
    "\n",
    "\n",
    "* When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.\n",
    "\n",
    "\n",
    "* Stacks of `Dense` layers with `relu` activations can solve a wide range of problems, and you will likely use them frequently.\n",
    "\n",
    "\n",
    "* In a binary classification problem (two output classes), your network should end with a `Dense` layer with 1 unit and a `sigmoid` activation, i.e. the output of your network should be a scalar between 0 and 1, encoding a probability. The loss function you should use is `binary_crossentropy`.\n",
    "\n",
    "\n",
    "* In a multiclass classification problem (`K` output classes), your network should end with a `Dense` layer with `K` units and a `softmax` activation, i.e. the output of your network should be a probability distribution. The loss function you should use is `categorical_crossentropy`. In addition, you need to one-hot encode the labels before using them for training the network.\n",
    "\n",
    "\n",
    "* In a regression problem, your network should end with a `Dense` layer with **no** activation, i.e. the output of your network should be unbounded. The loss function you should use is `mean_squared_error`. \n",
    "\n",
    "\n",
    "* The `adam` optimizer is generally a good enough choice of optimizer, whatever your problem. That's one less thing for you to worry  about.\n",
    "\n",
    "\n",
    "In the next lesson, you will acquire a more formal understanding of some of the concepts you have encountered in these first examples, such as data preprocessing, model evaluation, and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
