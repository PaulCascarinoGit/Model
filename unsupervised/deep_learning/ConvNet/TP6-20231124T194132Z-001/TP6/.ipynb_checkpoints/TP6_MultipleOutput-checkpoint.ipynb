{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"colab":{"name":"TP6_MultipleOutput-checkpoint.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"680PwpoLY0q0"},"source":["# TP 6 - Image Classification with Convolutional Neural Networks\n","\n","In this assignment, you will build a ConvNet for solving an image classification problem. You will be working on a fashion/clothing dataset, where the goal is to categorize the items according to two criteria:\n"," \n"," - **Clothing type**: Shirts, dresses, jeans, shoes.\n"," - **Color**: Red, blue, black.\n"," \n","![clothing_dataset.jpg](attachment:clothing_dataset.jpg)\n","\n","You will tackle this problem in incremental steps:\n"," \n"," 1. build a **multi-class classifier**, where each pair of clothing type and color is a different class;\n"," \n"," - build a **multi-label classifier**, where your ConvNet has a different output for each possible sub-class (shirt, dress, jeans, shoes, red, blue, black), and they can be activated simultaneously in order to predict the clothing type and color.\n"," \n"," - build a **multi-output classifier**, where your ConvNet has two output heads, one for predicting the clothing type, and the other for predicting the color.\n"," \n","This assignment is open-ended. Your goal is to maximize the performance of your ConvNets by using all the tricks you have learned in the course."]},{"cell_type":"code","metadata":{"id":"S6Ad8mWlY0ra","executionInfo":{"status":"ok","timestamp":1616139100902,"user_tz":-60,"elapsed":5097,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras import utils, layers, models, optimizers\n","from keras.preprocessing import image\n","\n","from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3NM4KAcY0rd"},"source":["## 1. Fashion Dataset\n","\n","The dataset consists of 2525 images across seven combinations of clothing type and color:\n","\n"," - Black jeans (344 images)\n"," - Black shoes (358 images)\n"," - Blue dress (386 images)\n"," - Blue jeans (356 images)\n"," - Blue shirt (369 images)\n"," - Red dress (380 images)\n"," - Red shirt (332 images)\n","\n","These images were downloaded via the Bing Image Search API. The entire process of downloading the images and manually removing the irrelevant ones for each of the seven combinations took approximately 30 minutes. "]},{"cell_type":"markdown","metadata":{"id":"454IkggAY0r2"},"source":["#### Data Preprocessing\n","\n","As you already know, data should be formatted into appropriately pre-processed floating-point tensors before being fed into your networks. Currently, your data sits on a drive as JPEG files, so the steps for getting it into a network are roughly:\n","* Read the picture files.\n","* Decode the JPEG content to RBG grids of pixels.\n","* Convert these into floating-point tensors.\n","* Rescale the pixel values (between 0 and 255) to the [0, 1] interval (neural networks prefer to deal with small input values).\n","\n","It may seem a bit daunting, but thankfully Keras has utilities to take care of these steps automatically. Keras has a module with image processing helper tools, located at `keras.preprocessing.image`. In particular, it contains the class `ImageDataGenerator` which allows you to quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. Read the [doc](https://keras.io/preprocessing/image/) for more information.\n","\n","**Tips:** If you want to use data augmentation at a later time, come back here and create a new `ImageDataGenerator` with the appropriate parameters."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"bb4725d8f6cae40f7831be54c2e37890","grade":true,"grade_id":"cell-433b110c4a86bd6b","locked":false,"points":0,"schema_version":1,"solution":true},"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"C9GBwG10Y0r6","executionInfo":{"status":"error","timestamp":1616139139443,"user_tz":-60,"elapsed":1257,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}},"outputId":"1edd0cad-cfb5-4687-ae10-31da6393a76e"},"source":["# target image size\n","n_pix = 40\n","\n","# All images will be rescaled by 1./255\n","datagen = image.ImageDataGenerator(rescale=1./255, validation_split=0.2)\n","\n","# train set\n","train_generator = datagen.flow_from_directory(\"fashion_dataset\", subset='training', target_size=(n_pix,n_pix), batch_size=32)\n","\n","# validation set\n","valid_generator = datagen.flow_from_directory(\"fashion_dataset\", subset='validation', target_size=(n_pix,n_pix), batch_size=501)\n"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f1a00adbfa1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fashion_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_pix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_pix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   def flow_from_dataframe(self,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fashion_dataset'"]}]},{"cell_type":"markdown","metadata":{"id":"so0rhdOBY0r9"},"source":["Let's take a look at the output of these generators. They yield batches of 40x40 RGB images and their corresponding labels, with 32 samples in each batch. Note that a generator yields these batches indefinitely: it just loops endlessly over the images present in the target folder."]},{"cell_type":"code","metadata":{"id":"y8sIrigYY0sA"},"source":["for data_batch, labels_batch in train_generator:\n","    \n","    print('data batch shape:', data_batch.shape)\n","    print('data label shape:', labels_batch.shape)\n","    \n","    plt.imshow(data_batch[0])\n","    plt.show()\n","    \n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_GuRx7SY0sD"},"source":["The labels are one-hot encoded, as shown in the cell below."]},{"cell_type":"code","metadata":{"id":"SHO3kngRY0sE"},"source":["for key, value in train_generator.class_indices.items():\n","    print(utils.to_categorical(value, num_classes=7), \"=\", value, \"-->\", key)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i76tCSkBY0sI"},"source":["## 2. Multi-Class Classification\n","\n","**Exercise:** Build a ConvNet for multi-class classification. It must predict the 7 classes available in the dataset.\n","\n","- Define the architecture of your ConvNet.\n","\n","- Fit the ConvNet on the training set.\n","\n","- Evaluate its performance on the validation set.\n","\n","- Test your trained ConvNet on the images in the folder `fashion_examples`.\n","\n","**Tips:**\n","\n"," - Remember that the structure of a ConvNet starts off with a stack of alternated `Conv2D => Relu` and `MaxPooling2D` layers, which are then followed by a `Flatten` layer and several `Dense` layers. \n"," \n"," \n"," - Use a **small network**, otherwise you will not be able to train it on your computer.\n"," \n"," \n"," - Since you are dealing with multi-class classification, you end the network with a `softmax` activation. The number of units in this last layer is equal to the number of classes to predict. Each output will encode the probability that the network is looking at one of the classes.\n"," \n"," \n"," - For multi-class classification, the loss function is `categorical_crossentropy`."]},{"cell_type":"markdown","metadata":{"id":"8PgwsqTgY0sN"},"source":["#### Depth-wise Separable Convolution\n","\n","There’s a layer you can use as a drop-in replacement for `Conv2D` that will make your model lighter (fewer trainable weight parameters) and faster (fewer floating-point operations), and cause it to perform a few percentage points better on its task. That is precisely what the depthwise separable convolution layer does, which is called\n","`SeparableConv2D` in Keras. \n","\n","This layer performs a spatial convolution on each channel of its input, independently, before mixing output channels via a pointwise convolution (a 1x1 convolution). This is equivalent to separating the learning\n","of spatial features and the learning of channel-wise features, which makes a lot of\n","sense if you assume that spatial locations in the input are highly correlated, but different\n","channels are fairly independent. It requires significantly fewer parameters and\n","involves fewer computations, thus resulting in smaller, speedier models. And because\n","it’s a more representationally efficient way to perform convolution, it tends to learn\n","better representations using less data, resulting in better-performing models.\n","\n","These advantages become especially important when you’re training small models\n","from scratch on limited data. For instance, here’s how you can build a lightweight,\n","depthwise separable convnet for an image-classification task (softmax categorical classification)\n","on a small dataset."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"49e07c12afd9f2e9331cefa79c030875","grade":true,"grade_id":"cell-766d74eac91df83d","locked":false,"points":0,"schema_version":1,"solution":true},"id":"qgoPeOmiY0sm"},"source":["# Define the network\n","model = models.Sequential()\n","model.add(...)     # ADD CODE HERE\n","...                # ADD CODE HERE\n","model.add(...)     # ADD CODE HERE\n","model.compile(...) # ADD CODE HERE\n","\n","# Train the network. Hint: use .fit_generator(...)\n","history = ... # ADD CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6bhbBYlY0sn"},"source":["# Get the training info\n","loss     = history.history['loss']\n","val_loss = history.history['val_loss']\n","acc      = history.history['acc']\n","val_acc  = history.history['val_acc']\n","\n","# Visualize the history plots\n","plt.figure()\n","plt.plot(loss, 'b', label='Training loss')\n","plt.plot(val_loss, 'm', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()\n","plt.figure()\n","plt.plot(acc, 'b', label='Training acc')\n","plt.plot(val_acc, 'm', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nneZL0pTY0sn"},"source":["**Expected Output**:\n","\n","<table>\n","    <tr>\n","        <td>\n","            Accuracy on validation set\n","        </td>\n","        <td>\n","           around 95%\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"3d4ade13b72b6417449af2b5416b5766","grade":true,"grade_id":"cell-ed7fa4068153e180","locked":false,"points":0,"schema_version":1,"solution":true},"id":"9lSg2q73Y0sp"},"source":["# Get the class names from the training set\n","classes = list(train_generator.class_indices.keys())\n","\n","# Read the test images\n","test_generator = datagen.flow_from_directory(\"fashion_examples\", class_mode=None, target_size=(n_pix,n_pix), batch_size=9)\n","test_images = next(test_generator)\n","\n","# Test the network. Hint: use .predict() and .argmax()\n","preds  = None # ADD CODE HERE\n","labels = None # ADD CODE HERE\n","\n","# Visualize the predictions\n","f, ax = plt.subplots(3, 3, figsize=(12,12))\n","for i, img in enumerate(test_images):\n","    ax.flat[i].imshow(img)\n","    ax.flat[i].axis(\"off\")\n","    ax.flat[i].set_title(classes[labels[i]])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CCF3bmieY0sq"},"source":["Analyze the errors made by your ConvNet on the test images. \n","\n","- A fraction of them is due to images classified in the wrong category. This kind of errors can be corrected by training a deeper ConvNet, and by reducing the downsampling factor on the input images (we drastically reduced the image resolution to keep the ConvNet small). \n","\n","\n","- However, there are some images that your ConvNet can never get right, such as the black dress, the red shoes, and the blue shoes. This happens because there is no example in the training set that corresponds to black dresses, red shoes, or blue shoes. Consequently, the model does not know how to classify them. But the ConvNet is still trained to distinguish colors and clothing types. So, how can you exploit this intrinsic knowlenge of your ConvNet to correctly classify new categories of clothing? Let's find out!"]},{"cell_type":"markdown","metadata":{"id":"FYHUxNNaY0ss"},"source":["## 3. Multi-Class vs Multi-Label\n","\n","In **multi-class** classification, the model provides one separate output for each class in the training set. This is exactly what you did earlier: you built a ConvNet with as many outputs as the available classes. The mapping between the network outputs and the 7 classes in the fashion dataset is as follows:\n"," - Output 1 = Black Jeans \n"," - Output 2 = Black Shoes \n"," - Output 3 = Blue Dress \n"," - Output 4 = Blue Jeans \n"," - Output 5 = Blue Shirt \n"," - Output 6 = Red Dress \n"," - Output 7 = Red Shirt\n"," \n","The cell below shows the one-hot encoded labels for each class."]},{"cell_type":"code","metadata":{"id":"DxwDsNFoY0st"},"source":["for key, value in train_generator.class_indices.items():\n","    print(utils.to_categorical(value, num_classes=7), \"-->\", key)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pAsM6seY0st"},"source":["In **multi-label** classification, the model uses a different output convention: it provides one seperate output for each sub-class in the training set, provided that the samples belong to more than one category. For example, the fashion dataset includes two categories with several sub-classes:\n","\n","- **Category 1** = Clothing type\n","   - *Sub-class 1.1* = Jeans\n","   - *Sub-class 1.2* = Shoes\n","   - *Sub-class 1.3* = Dress\n","   - *Sub-class 1.4* = Shirt\n","   \n","   \n","- **Category 2** = Clothing color\n","   - *Sub-class 2.1* = Black\n","   - *Sub-class 2.2* = Blue\n","   - *Sub-class 2.3* = Red\n","   \n","Hence, the multi-label approach implies that the mapping between the network outputs and the 7 sub-classes in the fashion dataset is as follows:\n"," - Output 1 = Black \n"," - Output 2 = Blue\n"," - Output 3 = Dress \n"," - Output 4 = Jeans \n"," - Output 5 = Red\n"," - Output 6 = Shirt\n"," - Output 7 = Shoes\n","\n","The key difference here is that an item can be associated to multiple sub-classes. The cell below shows the one-hot encoded labels for the fashion dataset. Remark the difference with respect to the multi-class approach!\n","\n","**Note:** It is just a coincidence that the number of sub-classes is 7. This number is **not** related to multi-class classification, in which the classes were also 7."]},{"cell_type":"code","metadata":{"id":"5rrfKejeY0su"},"source":["labels = []\n","for key, value in train_generator.class_indices.items():\n","    labels.append( key.split(\"_\") )\n","\n","mlb = MultiLabelBinarizer()\n","multi_labels = mlb.fit_transform(labels)\n","\n","for i in range(multi_labels.shape[0]):\n","    tags = mlb.inverse_transform(multi_labels[[i]])\n","    print(multi_labels[i], \"-->\", \" \".join(*tags))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GWuDCUwY0sv"},"source":["With a multi-label encoding, you can represent all the possible combinations of clothing types and colors."]},{"cell_type":"code","metadata":{"id":"6bxVsic3Y0sv"},"source":["new_labels = [\"black_dress\", \"blue_shoes\", \"red_shoes\"]\n","\n","multi = mlb.transform([l.split(\"_\") for l in new_labels])\n","\n","for i in range(len(new_labels)):\n","    print(multi[i], \"-->\", labels[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOIDWAcRY0sv"},"source":["The following class extends the Keras generator, so as to produce the multi-label one-hot encoding."]},{"cell_type":"code","metadata":{"id":"Jkre8cnIY0sw"},"source":["class MultilabelGenerator:\n","    \n","    def __init__(self, generator):\n","        self.generator = generator\n","        self.classes = np.array([key for key, value in generator.class_indices.items()])\n","        labels = [tag.split(\"_\") for tag in self.classes]\n","        self.mlb = MultiLabelBinarizer()\n","        self.mlb.fit(labels)\n","                \n","    def __next__(self):\n","        batch_x, batch_y = self.generator.next()\n","        idx = batch_y.argmax(axis=1)\n","        batch_y = self.classes[idx]\n","        batch_y = [tag.split(\"_\") for tag in batch_y]\n","        batch_y = self.mlb.transform(batch_y)\n","        return batch_x, batch_y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SS1js6NAY0sw"},"source":["The code below prepares the generators for multi-label classification."]},{"cell_type":"code","metadata":{"id":"I-E7a0I4Y0sx"},"source":["train_generator_multilabel = MultilabelGenerator(train_generator)\n","valid_generator_multilabel = MultilabelGenerator(valid_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u9Znae40Y0sy"},"source":["## 4. Multi-Label Classification\n","\n","**Exercise:** Build a ConvNet for multi-label classification. It must predict the 7 sub-classes in the fashion dataset.\n","\n","- Define the architecture of your ConvNet.\n","\n","- Fit the ConvNet on the training set.\n","\n","- Evaluate its performance on the validation set.\n","\n","- Test your trained ConvNet on the images in the folder `fashion_examples`.\n","\n","**Tips:**\n","\n"," - Since you are dealing with *multi-label* classification, you end the network with a `sigmoid` activation. The number of units in this last layer is equal to the number of sub-classes to predict. Each output will encode the probability that the network is looking at one of those sub-classes.\n"," \n"," \n"," - For multi-label classification, the loss function is `binary_crossentropy`."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"daef9fc9b63950c976515bfc0ab4fd57","grade":true,"grade_id":"cell-9a006da2da588790","locked":false,"points":0,"schema_version":1,"solution":true},"id":"PzpadovbY0sz"},"source":["# Define the network\n","model = models.Sequential()\n","# ADD CODE HERE\n","\n","# Train the network. Hint: use .fit_generator(...)\n","history = ... # ADD CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVEb9uhyY0s0"},"source":["# Get the training info\n","loss     = history.history['loss']\n","val_loss = history.history['val_loss']\n","acc      = history.history['acc']\n","val_acc  = history.history['val_acc']\n","\n","# Visualize the history plots\n","plt.figure()\n","plt.plot(loss, 'b', label='Training loss')\n","plt.plot(val_loss, 'm', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()\n","plt.figure()\n","plt.plot(acc, 'b', label='Training acc')\n","plt.plot(val_acc, 'm', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwgVtPtbY0s0"},"source":["**Expected Output**: \n","\n","<table>\n","    <tr>\n","        <td>\n","            Accuracy on validation set\n","        </td>\n","        <td>\n","           around 98%\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"48x8_3e3Y0s0"},"source":["# Test the network\n","preds = model.predict(test_images)\n","preds = preds > 0.7\n","\n","# Get the multi-label categories\n","tags = train_generator_multilabel.mlb.inverse_transform(preds)\n","\n","# Visualize the predictions\n","f, ax = plt.subplots(3, 3, figsize=(12,12))\n","for i, img in enumerate(test_images):\n","    ax.flat[i].imshow(img)\n","    ax.flat[i].axis(\"off\")\n","    ax.flat[i].set_title(\" \".join(tags[i]))  \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xbO4yfMeY0s1"},"source":["Analyze the errors made by your ConvNet on the test images of never-seen-before clothing (black dress, red shoes, blue shoes). Your ConvNet may only provide a partial prediction (e.g., the color), or no prediction at all. This happens because the output layer uses the SIGMOID activation, instead of SOFTMAX. Let's see how the network can be forced to always produce full predictions!"]},{"cell_type":"markdown","metadata":{"id":"aLZUQhhPY0s1"},"source":["## 5. Networks with arbitrary layouts\n","\n","So far, you have built networks with a linear sequence of layers, from input to output. It is however possible to define more complex networks. The Keras [Functional API](https://keras.io/getting-started/functional-api-guide/) is the way to go for defining arbitrary layouts, such as multi-output models, directed acyclic graphs, or models with shared layers. The functional API uses a sintax slightly different than the `Sequential` model. Let's start with some examples.\n","\n","### 5.1 Single input - Single output\n","\n","Here is an example of ConvNet with one output layer.\n","\n","```python\n","# Define the input layer. Think of this as a placeholder of your input!\n","X_input = layers.Input((28,28,1))\n","\n","# Stack the hidden layers\n","X = layers.Conv2D(8, (3,3), activation='relu')(X_input)\n","X = layers.BatchNormalization()(X)\n","X = layers.MaxPooling2D((2,2))(X)\n","X = layers.Flatten()(X)\n","\n","# Define the output layer\n","X = layers.Dense(10, activation='softmax')(X)\n","\n","# Create the single-output model. This is what you'll train!\n","model = models.Model(inputs = X_input, outputs = X)\n","```\n","\n","The next figure shows a standard model with sequential layers.\n","\n","![model_single_output.png](attachment:model_single_output.png)"]},{"cell_type":"markdown","metadata":{"id":"yUoiebudY0s1"},"source":["### 5.2 Single input - Multiple output\n","\n","Here is an example of ConvNet with two output layers.\n","\n","```python\n","# Define the input layer\n","X_input = layers.Input((28,28,1))\n","\n","# Stack the shared layers\n","X = layers.Conv2D(8, (3,3), activation='relu')(X_input)\n","X = layers.BatchNormalization()(X)\n","X = layers.MaxPooling2D((2,2))(X)\n","    \n","# Define the 1st head\n","X_head1 = layers.Conv2D(16, (3,3), activation='relu')(X)\n","X_head1 = layers.MaxPooling2D((2,2))(X_head1)\n","X_head1 = layers.Flatten()(X_head1)\n","X_head1 = layers.Dense(10, activation='softmax', name=\"Output_1\")(X_head1) # output 1\n","\n","# Define the 2nd head\n","X_head2 = layers.Conv2D(32, (5,5), activation='relu')(X)\n","X_head2 = layers.MaxPooling2D((2,2))(X_head2)\n","X_head2 = layers.Flatten()(X_head2)\n","X_head2 = layers.Dense(4, activation='softmax', name=\"Output_2\")(X_head2) # output 2\n","\n","# Create the multi-output model\n","model = models.Model(inputs = X_input, outputs = [X_head1,X_head2])\n","```"]},{"cell_type":"markdown","metadata":{"id":"-9RsywyhY0s3"},"source":["The next figure shows a model with multiple outputs.\n","\n","![model_multiple_output.png](attachment:model_multiple_output.png)"]},{"cell_type":"markdown","metadata":{"id":"bdH5opnUY0s4"},"source":["## 6. Multi-Label vs Multi-Output\n","\n","In **multi-output** classification, the model provides a different output vector for each category in the training set. For example, the fashion dataset includes two categories with several sub-classes:\n","\n","- **Category 1** = Clothing type\n","   - *Sub-class 1.1* = Jeans\n","   - *Sub-class 1.2* = Shoes\n","   - *Sub-class 1.3* = Dress\n","   - *Sub-class 1.4* = Shirt\n","   \n","   \n","- **Category 2** = Color\n","   - *Sub-class 2.1* = Black\n","   - *Sub-class 2.2* = Blue\n","   - *Sub-class 2.3* = Red\n","   \n","Hence, the network will have two output layers composed of 3 and 4 units, respectively. The mapping between the network outputs and the 7 sub-classes in the fashion dataset is as follows:\n","\n","- **Output layer 1** = Color\n","  - Unit 1 = Black \n","  - Unit 2 = Blue\n","  - Unit 3 = Red\n","  \n","- **Output layer 2** = Clothing type\n","  - Unit 1 = Jeans \n","  - Unit 2 = Dress\n","  - Unit 3 = Shirt\n","  - Unit 4 = Shoes\n","  \n","The key difference here is that an item is always associated to two outputs. The cell below shows the one-hot encoded labels for the fashion dataset. Remark the difference with respect to the multi-label approach!"]},{"cell_type":"code","metadata":{"id":"IQBdGC6kY0s4"},"source":["labels = np.array([key.split(\"_\") for key, value in train_generator.class_indices.items()])\n","\n","bin1 = LabelBinarizer()\n","bin2 = LabelBinarizer()\n","\n","tag1 = bin1.fit_transform(labels[:,0])\n","tag2 = bin2.fit_transform(labels[:,1])\n","\n","print(\" color      cloth\")\n","for i in range(len(labels)):\n","    print(tag1[i], \"|\", tag2[i], \"-->\", \" \".join(labels[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-b-7NEHY0s4"},"source":["The following class extends the Keras generator, so as to produce the multi-output one-hot encoding."]},{"cell_type":"code","metadata":{"id":"lPj60uRbY0s5"},"source":["class MultiOutputGenerator:\n","    \n","    def __init__(self, generator):\n","        self.generator = generator\n","        self.classes = np.array([key for key, value in generator.class_indices.items()])\n","        labels = np.array([tag.split(\"_\") for tag in self.classes])\n","        self.tag1 = LabelBinarizer()\n","        self.tag1.fit(labels[:,0])\n","        self.tag2 = LabelBinarizer()\n","        self.tag2.fit(labels[:,1])\n","                \n","    def __next__(self):\n","        batch_x, batch_y = self.generator.next()\n","        idx = batch_y.argmax(axis=1)\n","        batch_y = self.classes[idx]\n","        batch_y = np.array([tag.split(\"_\") for tag in batch_y])\n","        batch_y1 = self.tag1.transform(batch_y[:,0])\n","        batch_y2 = self.tag2.transform(batch_y[:,1])\n","        return batch_x, [batch_y1, batch_y2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I4_GJuxnY0s6"},"source":["The code below prepares the generators for multi-output classification."]},{"cell_type":"code","metadata":{"id":"VQgthGlpY0s6"},"source":["train_generator_multioutput = MultiOutputGenerator(train_generator)\n","valid_generator_multioutput = MultiOutputGenerator(valid_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y6quVvp1Y0s7"},"source":["## 7. Multi-Output Classification\n","\n","**Exercise:** Build a ConvNet for multi-output classification. It must predict the 2 categories in the fashion dataset.\n","\n","- Define the architecture of your ConvNet.\n","\n","- Fit the ConvNet on the training set.\n","\n","- Evaluate its performance on the validation set.\n","\n","- Test your trained ConvNet on the images in the folder `fashion_examples`.\n","\n","**Tips:**\n","\n"," - Since you are dealing with *multi-output* classification, you end the network with two output layers, both having a `softmax` activation. The number of units in these last layer is equal to the number of sub-classes in each category to predict. \n"," \n"," \n"," - For multi-output classification, the loss function is `categorical_crossentropy`.\n"," \n"," \n"," - To visualize the layout of your ConvNet, you can use the function `plot_model()` as follows:\n"," ```\n"," utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)\n"," ```\n"," This is particularly useful to debug the definition of your model. (*Note that you need to install the package \"graphviz\" to use this function.*)"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"f75efbfe955177f86c0e875eb0463f98","grade":true,"grade_id":"cell-0e0f163689753ebc","locked":false,"points":0,"schema_version":1,"solution":true},"id":"o9Z-Qk-MY0s7"},"source":["# Define the network. Hint: use the functional API\n","model = ... # ADD CODE HERE\n","\n","# Train the network. Hint: use .fit_generator(...)\n","history = ... # ADD CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQer15kiY0s8"},"source":["# Get the training info\n","loss     = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# Visualize the history plots\n","plt.figure()\n","plt.plot(loss, 'b', label='Training loss')\n","plt.plot(val_loss, 'm', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qpp1YH0QY0s9"},"source":["**Expected Output**:\n","\n","<table>\n","    <tr>\n","        <td>\n","        </td>\n","        <td>\n","            Valid. accuracy\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","            Color\n","        </td>\n","        <td>\n","            around 99%\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","            Type\n","        </td>\n","        <td>\n","            around 97%\n","        </td>\n","    </tr>\n","</table>\n","\n"]},{"cell_type":"code","metadata":{"id":"hhp_PffRY0s-"},"source":["# Test the network\n","pred1, pred2 = model.predict(test_images)\n","color = pred1.argmax(axis=1)\n","cloth = pred2.argmax(axis=1)\n","\n","# Get the multi-output categories\n","tag1 = train_generator_multioutput.tag1.inverse_transform(pred1)\n","tag2 = train_generator_multioutput.tag2.inverse_transform(pred2)\n","\n","# Visualize the predictions\n","f, ax = plt.subplots(3, 3, figsize=(12,12))\n","for i, img in enumerate(test_images):\n","    ax.flat[i].imshow(img)\n","    ax.flat[i].axis(\"off\")\n","    ax.flat[i].set_title(\" \".join([tag1[i],tag2[i]]))  \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8cIbqN4Y0s-"},"source":["Analyze the errors made by your ConvNet on the test images of never-seen-before clothing (black dress, red shoes, blue shoes):\n","\n","- Your ConvNet is able to correctly predict the color.\n","\n","\n","- Depending on the architecture of your ConvNet, the prediction of the clothing type may still be wrong. "]},{"cell_type":"markdown","metadata":{"id":"87YwZaXkY0s_"},"source":["## 7. Conclusion\n","\n","**Remember:** There are the four steps for building a neural network in Keras: \n"," 1. Stack the layers with the `Sequential` class or the functional API\n"," - Compile the model\n"," - Fit/Train\n"," - Evaluate/Test"]}]}